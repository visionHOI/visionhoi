from collections import OrderedDict
from typing import Tuple, Union, List

import torch
import torch.nn as nn
import packaging

from CLIP.simple_tokenizer import SimpleTokenizer as _Tokenizer


class MetaNet(nn.Module):
    """
    CoCoOp 元网络：将图像特征映射为 ctx 偏置向量。

    工作原理（通俗版）：
      - 输入：当前图像的全局特征向量（512 维）
      - 输出：一个「偏置」向量（ctx_dim 维，同样是 512 维）
      - 使用方式：把这个偏置加到原有的可学习 ctx 向量上
        → 原来 ctx 是固定的「通用提示词」
        → 加上偏置后变成「根据当前图像定制的提示词」

    为什么是 vis_dim//16 的隐层？
      - 保持轻量（参数少），不抢主干的梯度
      - 类似 Adapter 的瓶颈设计
    """
    def __init__(self, vis_dim: int, ctx_dim: int):
        super().__init__()
        hidden = max(vis_dim // 16, 32)          # 瓶颈维度，至少 32
        self.net = nn.Sequential(
            nn.Linear(vis_dim, hidden),
            nn.ReLU(inplace=True),
            nn.Linear(hidden, ctx_dim),          # 输出与 ctx 同维度
        )
        # 初始化为零：训练开始时 MetaNet 输出 ≈ 0，不破坏原有 CoOp 表现
        nn.init.zeros_(self.net[-1].weight)
        nn.init.zeros_(self.net[-1].bias)

    def forward(self, image_feature: torch.Tensor) -> torch.Tensor:
        """
        Args:
            image_feature: (1, vis_dim)  当前图像的归一化全局特征
        Returns:
            bias: (1, ctx_dim)  用于偏移 ctx 的向量
        """
        return self.net(image_feature)           # (1, ctx_dim)


class TextEncoder(nn.Module):
    def __init__(self, clip_model):
        super().__init__()
        self.transformer = clip_model.transformer
        self.positional_embedding = clip_model.positional_embedding
        self.ln_final = clip_model.ln_final
        self.text_projection = clip_model.text_projection
        self.dtype = clip_model.dtype

    def forward(self, prompts, tokenized_prompts):
        x = prompts + self.positional_embedding.type(self.dtype)
        x = x.permute(1, 0, 2)  # NLD -> LND
        x = self.transformer(x)
        x = x.permute(1, 0, 2)  # LND -> NLD
        x = self.ln_final(x).type(self.dtype)

        # x.shape = [batch_size, n_ctx, transformer.width]
        # take features from the eot embedding (eot_token is the highest number in each sequence)
        x = x[torch.arange(x.shape[0]), tokenized_prompts.argmax(dim=-1)] @ self.text_projection

        return x

class PromptLearner(nn.Module):
    def __init__(self, args, classnames, clip_model):
        super().__init__()
        self.args = args
        n_cls = len(classnames)
        n_ctx = args.N_CTX # cfg.TRAINER.COOP.N_CTX
        ctx_init = args.CTX_INIT ## cfg.TRAINER.COOP.CTX_INIT
        dtype = clip_model.dtype
        ctx_dim = clip_model.ln_final.weight.shape[0]
        # clip_imsize = clip_model.visual.input_resolution
        # cfg_imsize = cfg.INPUT.SIZE[0]
        # assert cfg_imsize == clip_imsize, f"cfg_imsize ({cfg_imsize}) must equal to clip_imsize ({clip_imsize})"

        if ctx_init:
            # use given words to initialize context vectors
            ctx_init = ctx_init.replace("_", " ")
            n_ctx = len(ctx_init.split(" "))
            prompt = tokenize(ctx_init)
            with torch.no_grad():
                embedding = clip_model.token_embedding(prompt).type(dtype)
            ctx_vectors = embedding[0, 1 : 1 + n_ctx, :]
            prompt_prefix = ctx_init
        else:
            # random initialization
            if args.CSC: # cfg.TRAINER.COOP.CSC:
                print("Initializing class-specific contexts")
                ctx_vectors = torch.empty(n_cls, n_ctx, ctx_dim, dtype=dtype)
            else:
                print("Initializing a generic context")
                ctx_vectors = torch.empty(n_ctx, ctx_dim, dtype=dtype)
            nn.init.normal_(ctx_vectors, std=0.02)
            prompt_prefix = " ".join(["X"] * n_ctx)

        print(f'Initial context: "{prompt_prefix}"')
        print(f"Number of context words (tokens): {n_ctx}")

        self.ctx = nn.Parameter(ctx_vectors)  # to be optimized

        # ── CoCoOp：可选的 MetaNet ──────────────────────────────────────────
        # use_cocoop=True 时才创建，否则与原有 CoOp 完全相同，不引入额外参数
        self.use_cocoop = getattr(args, 'use_cocoop', False)
        if self.use_cocoop:
            # vis_dim：CLIP 视觉编码器的输出维度（ViT-B/16 → 512）
            vis_dim = clip_model.visual.output_dim
            self.meta_net = MetaNet(vis_dim, ctx_dim)
            print(f"[CoCoOp] MetaNet enabled: vis_dim={vis_dim} → ctx_dim={ctx_dim}")
        # ────────────────────────────────────────────────────────────────────


        # classnames = [name.replace("_", " ") for name in classnames]
        name_lens = [len(_tokenizer.encode(name)) for name in classnames]
        prompts = [prompt_prefix + " " + name + "." for name in classnames]

        tokenized_prompts = torch.cat([tokenize(p) for p in prompts])
        with torch.no_grad():
            embedding = clip_model.token_embedding(tokenized_prompts).type(dtype)

        # These token vectors will be saved when in save_model(),
        # but they should be ignored in load_model() as we want to use
        # those computed using the current class names
        self.register_buffer("token_prefix", embedding[:, :1, :])  # SOS
        self.register_buffer("token_suffix", embedding[:, 1 + n_ctx :, :])  # CLS, EOS

        self.n_cls = n_cls
        self.n_ctx = n_ctx
        self.tokenized_prompts = tokenized_prompts  # torch.Tensor
        self.name_lens = name_lens
        self.class_token_position = args.CLASS_TOKEN_POSITION  # cfg.TRAINER.COOP.CLASS_TOKEN_POSITION

    def forward(self, image_feature=None):
        """
        Args:
            image_feature: (1, vis_dim) 或 None
              - None        → 普通 CoOp 行为（固定 ctx，与原来完全一致）
              - Tensor(1,D) → CoCoOp 行为：用 MetaNet 生成偏置叠加到 ctx 上

        通俗解释：
          每次 forward，ctx 要被拼成 n_cls 份（每个类别一份）。
          CoCoOp 在拼之前，先用 MetaNet 算出一个针对「当前图像」的偏置，
          加到 ctx 上，让每张图对应的文本提示词稍有不同。
        """
        ctx = self.ctx

        # ── CoCoOp 核心：给 ctx 叠加图像条件偏置 ──────────────────────────
        if self.use_cocoop and image_feature is not None:
            # bias 形状: (1, ctx_dim)  ← MetaNet 输出
            bias = self.meta_net(image_feature)          # (1, ctx_dim)

            if ctx.dim() == 2:
                # 共享 ctx: (n_ctx, ctx_dim)
                # bias.unsqueeze(0) → (1, 1, ctx_dim)，广播到所有 n_ctx 位置
                ctx = ctx + bias.unsqueeze(0)            # (n_ctx, ctx_dim)
                ctx = ctx.unsqueeze(0).expand(self.n_cls, -1, -1)
            else:
                # 类特异 ctx: (n_cls, n_ctx, ctx_dim)
                # bias.unsqueeze(1) → (1, 1, ctx_dim)，广播到所有类和所有位置
                ctx = ctx + bias.unsqueeze(1)            # (n_cls, n_ctx, ctx_dim)
        else:
            # 原有 CoOp 逻辑，完全不变
            if ctx.dim() == 2:
                ctx = ctx.unsqueeze(0).expand(self.n_cls, -1, -1)
        # ────────────────────────────────────────────────────────────────────

        prefix = self.token_prefix
        suffix = self.token_suffix

        if self.class_token_position == "end":
            prompts = torch.cat(
                [
                    prefix,  # (n_cls, 1, dim)
                    ctx,     # (n_cls, n_ctx, dim)
                    suffix,  # (n_cls, *, dim)
                ],
                dim=1,
            )

        elif self.class_token_position == "middle":
            half_n_ctx = self.n_ctx // 2
            prompts = []
            for i in range(self.n_cls):
                name_len = self.name_lens[i]
                prefix_i = prefix[i : i + 1, :, :]
                class_i = suffix[i : i + 1, :name_len, :]
                suffix_i = suffix[i : i + 1, name_len:, :]
                ctx_i_half1 = ctx[i : i + 1, :half_n_ctx, :]
                ctx_i_half2 = ctx[i : i + 1, half_n_ctx:, :]
                prompt = torch.cat(
                    [
                        prefix_i,     # (1, 1, dim)
                        ctx_i_half1,  # (1, n_ctx//2, dim)
                        class_i,      # (1, name_len, dim)
                        ctx_i_half2,  # (1, n_ctx//2, dim)
                        suffix_i,     # (1, *, dim)
                    ],
                    dim=1,
                )
                prompts.append(prompt)
            prompts = torch.cat(prompts, dim=0)

        elif self.class_token_position == "front":
            prompts = []
            for i in range(self.n_cls):
                name_len = self.name_lens[i]
                prefix_i = prefix[i : i + 1, :, :]
                class_i = suffix[i : i + 1, :name_len, :]
                suffix_i = suffix[i : i + 1, name_len:, :]
                ctx_i = ctx[i : i + 1, :, :]
                prompt = torch.cat(
                    [
                        prefix_i,  # (1, 1, dim)
                        class_i,   # (1, name_len, dim)
                        ctx_i,     # (1, n_ctx, dim)
                        suffix_i,  # (1, *, dim)
                    ],
                    dim=1,
                )
                prompts.append(prompt)
            prompts = torch.cat(prompts, dim=0)

        else:
            raise ValueError

        return prompts

class CustomCLIP(nn.Module):
    def __init__(self, args, classnames, clip_model):
        super().__init__()
        self.prompt_learner = PromptLearner(args, classnames, clip_model)
        self.tokenized_prompts = self.prompt_learner.tokenized_prompts
        self.image_encoder = clip_model.visual
        self.text_encoder = TextEncoder(clip_model)
        self.logit_scale = clip_model.logit_scale
        self.dtype = clip_model.dtype

    def forward(self, image):
        image_features = self.image_encoder(image.type(self.dtype))

        prompts = self.prompt_learner()
        tokenized_prompts = self.tokenized_prompts
        text_features = self.text_encoder(prompts, tokenized_prompts)

        image_features = image_features / image_features.norm(dim=-1, keepdim=True)
        text_features = text_features / text_features.norm(dim=-1, keepdim=True)

        logit_scale = self.logit_scale.exp()
        logits = logit_scale * image_features @ text_features.t()
        raise ValueError
        return logits

_tokenizer = _Tokenizer()

def tokenize(texts: Union[str, List[str]], context_length: int = 77, truncate: bool = False, return_sot=True) -> Union[torch.IntTensor, torch.LongTensor]:
    """
    Returns the tokenized representation of given input string(s)

    Parameters
    ----------
    texts : Union[str, List[str]]
        An input string or a list of input strings to tokenize

    context_length : int
        The context length to use; all CLIP models use 77 as the context length

    truncate: bool
        Whether to truncate the text in case its encoding is longer than the context length

    Returns
    -------
    A two-dimensional tensor containing the resulting tokens, shape = [number of input strings, context_length].
    We return LongTensor when torch version is <1.8.0, since older index_select requires indices to be long.
    """
    if isinstance(texts, str):
        texts = [texts]

    sot_token = _tokenizer.encoder["<|startoftext|>"]
    eot_token = _tokenizer.encoder["<|endoftext|>"]
    # all_tokens = [[sot_token] + _tokenizer.encode(text) + [eot_token] for text in texts]

    # if return_sot:
    #     all_tokens = [[sot_token] + _tokenizer.encode(text) for text in texts]
    # else:
    #     all_tokens = [_tokenizer.encode(text) + [eot_token] for text in texts]
    all_tokens = [[sot_token] + _tokenizer.encode(text) + [eot_token] for text in texts]
    if packaging.version.parse(torch.__version__) < packaging.version.parse("1.8.0"):
        result = torch.zeros(len(all_tokens), context_length, dtype=torch.long)
    else:
        result = torch.zeros(len(all_tokens), context_length, dtype=torch.int)

    for i, tokens in enumerate(all_tokens):
        if len(tokens) > context_length:
            if truncate:
                tokens = tokens[:context_length]
                tokens[-1] = eot_token
            else:
                raise RuntimeError(f"Input {texts[i]} is too long for context length {context_length}")
        result[i, :len(tokens)] = torch.tensor(tokens)

    return result

