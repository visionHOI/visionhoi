对我的工作里面readme的补充，主要说我的工作内容和实现方式。

## 修改动机：
这版本主要实现的就是显式学习场景信息。clip的源代码中是把cls token直接丢掉了，原因是ho token里面因为是看的图片的某一小块所以本身就有那部分的场景信息，但是它是和hoi信息合在一起的，没有把他分开显式处理。而直接把cls token加进去又会产生“特征冗余“与”对齐冲突”问题。

### llm描述的特征冗余与对齐冲突：
 一、 什么是特征冗余 (Feature Redundancy)？

核心逻辑： 在 Transformer 的深层网络中，信息已经高度混合，“你中有我，我中有你”。

混合机制：你在代码中看到的 `ResidualAttentionBlock` 使用了自注意力机制（Self-Attention）。这意味着，在每一层中，`ho_tokens`（交互 token）都会去“查询”并聚合 `class_embedding`（全局 token）的信息，反之亦然。

同质化现象： 经过十几层（如 CLIP ResNet50 有 16 层，ViT-B 有 12 层）的交互后，`ho_tokens` 实际上已经隐含了 `class_embedding` 中的全局上下文。

冗余后果：如果你最后强行把它们拼接（Concat）或相加，本质上是在做“重复信号放大”。这不会增加新的有效信息量，反而可能引入噪声，导致模型在反向传播时权重更新方向不明确（梯度混乱）。


 二、 什么是对齐冲突 (Alignment Conflict)？

核心逻辑： 文本（Text Prompt）的粒度与图像 Token 的粒度不匹配。

文本端的粒度： 在 HOI 任务中，你的文本 Prompt 通常是非常具体的动作描述，例如 _"holding a cup"_（拿着杯子）。这是一个**局部、细粒度**的语义。

[CLS] 的粒度：`class_embedding` 的设计初衷是表征整张图，它包含的是**全局、粗粒度**的信息（例如：“厨房背景”、“光照”、“所有物体的大概分布”）。
冲突爆发：
    - 如果你把 `[CLS]`（厨房背景）强行融合进 `ho_tokens`（拿杯子动作）。
    - 最终生成的图像特征向量 $V_{final}$ 会向“厨房”这个语义偏移。
    - 而在 CLIP 的共享特征空间里，文本向量 $T_{action}$ 代表“拿着杯子”。
    - **结果：** $V_{final}$ 和 $T_{action}$ 的余弦相似度（Cosine Similarity）反而降低了，因为 $V_{final}$ 被“背景噪声”拉偏了。这就是对齐冲突。
    - 


## 修改思路：
但是直接把cls丢掉背景无法被显式考虑，对于背景影响这一因素的可解释性不好，所以这版本主要的目标就是想找一个cls token去融合进他算的图像分数里。直接把这些场景分类出来感觉面对zero shot就更不好用了，所以采用的是让他自己去学习适当的方式去把他融合进去。

## 目前效果：
目前按照上面的实现已经训练了七轮，效果见readme。

## 下一步思路：
目前是把一张图的单一场景作为学习该如何考虑它的影响，但下一步的考虑方式是场景+单一hoi（eg：图中是厨房，一个人在喝水，一个人在看手机，目前是只判断厨房这个场景如何考虑影响，之后想做到把厨房+喝水进行学习），就一定会产生过拟合的问题。目前的考虑是加入低秩分解防止过拟合。